{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text],dtype = np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "\n",
    "    x = sequence[0: num_batches*mini_batch_length]\n",
    "    y = sequence[1: num_batches*mini_batch_length + 1]\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps:(b+1)*num_steps],\n",
    "               data_y[:, b*num_steps:(b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64,\n",
    "                 num_steps=100, lstm_size=128,\n",
    "                 num_layers=1, learning_rate=0.001,\n",
    "                 keep_prob=0.5, grad_clip=5,\n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                              name='tf_keepprob')\n",
    "\n",
    "        # One-hot encoding:\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        ### Build the multi-layer RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "            for _ in range(self.num_layers)])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(\n",
    "                    batch_size, tf.float32)\n",
    "\n",
    "        ## Run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                    cells, x_onehot,\n",
    "                    initial_state=self.initial_state)\n",
    "        \n",
    "        print('  << lstm_outputs  >>', lstm_outputs)\n",
    "\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "                    lstm_outputs,\n",
    "                    shape=[-1, self.lstm_size],\n",
    "                    name='seq_output_reshaped')\n",
    "\n",
    "        logits = tf.layers.dense(\n",
    "                    inputs=seq_output_reshaped,\n",
    "                    units=self.num_classes,\n",
    "                    activation=None,\n",
    "                    name='logits')\n",
    "\n",
    "        proba = tf.nn.softmax(\n",
    "                    logits,\n",
    "                    name='probabilities')\n",
    "\n",
    "        y_reshaped = tf.reshape(\n",
    "                    y_onehot,\n",
    "                    shape=[-1, self.num_classes],\n",
    "                    name='y_reshaped')\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits,\n",
    "                        labels=y_reshaped),\n",
    "                    name='cost')\n",
    "\n",
    "        # Gradient clipping to avoid \"exploding gradients\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(cost, tvars),\n",
    "                    self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y,\n",
    "              num_epochs, ckpt_dir='./model/'):\n",
    "        ## Create the checkpoint directory\n",
    "        ## if it does not exists\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # Train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                ## Mini-batch generator:\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0' : self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op',\n",
    "                                self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (\n",
    "                              epoch + 1, num_epochs,\n",
    "                              iteration, batch_cost))\n",
    "\n",
    "                ## Save the trained model\n",
    "                self.saver.save(\n",
    "                        sess, os.path.join(\n",
    "                            ckpt_dir, 'language_modeling.ckpt'))\n",
    "\n",
    "    def sample(self, output_length,\n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess,\n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            ## 1: run the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            ## 2: run the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-adb43cfdbd04>:54: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-adb43cfdbd04>:54: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-adb43cfdbd04>:63: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/paul.tgr/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/paul.tgr/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-adb43cfdbd04>:76: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-adb43cfdbd04>:89: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints,\n",
    "                             batch_size,\n",
    "                               num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Iteration 10| Training loss: 3.8463\n",
      "Epoch 1/100 Iteration 20| Training loss: 3.3570\n",
      "Epoch 2/100 Iteration 30| Training loss: 3.2839\n",
      "Epoch 2/100 Iteration 40| Training loss: 3.2481\n",
      "Epoch 2/100 Iteration 50| Training loss: 3.2278\n",
      "Epoch 3/100 Iteration 60| Training loss: 3.2106\n",
      "Epoch 3/100 Iteration 70| Training loss: 3.1942\n",
      "Epoch 4/100 Iteration 80| Training loss: 3.1783\n",
      "Epoch 4/100 Iteration 90| Training loss: 3.1478\n",
      "Epoch 4/100 Iteration 100| Training loss: 3.1442\n",
      "Epoch 5/100 Iteration 110| Training loss: 3.1291\n",
      "Epoch 5/100 Iteration 120| Training loss: 3.0998\n",
      "Epoch 6/100 Iteration 130| Training loss: 3.0602\n",
      "Epoch 6/100 Iteration 140| Training loss: 3.0240\n",
      "Epoch 6/100 Iteration 150| Training loss: 3.0061\n",
      "Epoch 7/100 Iteration 160| Training loss: 2.9642\n",
      "Epoch 7/100 Iteration 170| Training loss: 2.9210\n",
      "Epoch 8/100 Iteration 180| Training loss: 2.8678\n",
      "Epoch 8/100 Iteration 190| Training loss: 2.8349\n",
      "Epoch 8/100 Iteration 200| Training loss: 2.7976\n",
      "Epoch 9/100 Iteration 210| Training loss: 2.7810\n",
      "Epoch 9/100 Iteration 220| Training loss: 2.7350\n",
      "Epoch 10/100 Iteration 230| Training loss: 2.6927\n",
      "Epoch 10/100 Iteration 240| Training loss: 2.6756\n",
      "Epoch 10/100 Iteration 250| Training loss: 2.6248\n",
      "Epoch 11/100 Iteration 260| Training loss: 2.6238\n",
      "Epoch 11/100 Iteration 270| Training loss: 2.5852\n",
      "Epoch 12/100 Iteration 280| Training loss: 2.5559\n",
      "Epoch 12/100 Iteration 290| Training loss: 2.5543\n",
      "Epoch 12/100 Iteration 300| Training loss: 2.5072\n",
      "Epoch 13/100 Iteration 310| Training loss: 2.5209\n",
      "Epoch 13/100 Iteration 320| Training loss: 2.4855\n",
      "Epoch 14/100 Iteration 330| Training loss: 2.4631\n",
      "Epoch 14/100 Iteration 340| Training loss: 2.4742\n",
      "Epoch 14/100 Iteration 350| Training loss: 2.4180\n",
      "Epoch 15/100 Iteration 360| Training loss: 2.4489\n",
      "Epoch 15/100 Iteration 370| Training loss: 2.4170\n",
      "Epoch 16/100 Iteration 380| Training loss: 2.4035\n",
      "Epoch 16/100 Iteration 390| Training loss: 2.4254\n",
      "Epoch 16/100 Iteration 400| Training loss: 2.3622\n",
      "Epoch 17/100 Iteration 410| Training loss: 2.4024\n",
      "Epoch 17/100 Iteration 420| Training loss: 2.3657\n",
      "Epoch 18/100 Iteration 430| Training loss: 2.3536\n",
      "Epoch 18/100 Iteration 440| Training loss: 2.3809\n",
      "Epoch 18/100 Iteration 450| Training loss: 2.3179\n",
      "Epoch 19/100 Iteration 460| Training loss: 2.3545\n",
      "Epoch 19/100 Iteration 470| Training loss: 2.3167\n",
      "Epoch 20/100 Iteration 480| Training loss: 2.3102\n",
      "Epoch 20/100 Iteration 490| Training loss: 2.3314\n",
      "Epoch 20/100 Iteration 500| Training loss: 2.2776\n",
      "Epoch 21/100 Iteration 510| Training loss: 2.3182\n",
      "Epoch 21/100 Iteration 520| Training loss: 2.2862\n",
      "Epoch 22/100 Iteration 530| Training loss: 2.2802\n",
      "Epoch 22/100 Iteration 540| Training loss: 2.3039\n",
      "Epoch 22/100 Iteration 550| Training loss: 2.2482\n",
      "Epoch 23/100 Iteration 560| Training loss: 2.3074\n",
      "Epoch 23/100 Iteration 570| Training loss: 2.2633\n",
      "Epoch 24/100 Iteration 580| Training loss: 2.2606\n",
      "Epoch 24/100 Iteration 590| Training loss: 2.2954\n",
      "Epoch 24/100 Iteration 600| Training loss: 2.2262\n",
      "Epoch 25/100 Iteration 610| Training loss: 2.2761\n",
      "Epoch 25/100 Iteration 620| Training loss: 2.2395\n",
      "Epoch 26/100 Iteration 630| Training loss: 2.2354\n",
      "Epoch 26/100 Iteration 640| Training loss: 2.2667\n",
      "Epoch 26/100 Iteration 650| Training loss: 2.1970\n",
      "Epoch 27/100 Iteration 660| Training loss: 2.2545\n",
      "Epoch 27/100 Iteration 670| Training loss: 2.2071\n",
      "Epoch 28/100 Iteration 680| Training loss: 2.2090\n",
      "Epoch 28/100 Iteration 690| Training loss: 2.2563\n",
      "Epoch 28/100 Iteration 700| Training loss: 2.1707\n",
      "Epoch 29/100 Iteration 710| Training loss: 2.2185\n",
      "Epoch 29/100 Iteration 720| Training loss: 2.1941\n",
      "Epoch 30/100 Iteration 730| Training loss: 2.1969\n",
      "Epoch 30/100 Iteration 740| Training loss: 2.2219\n",
      "Epoch 30/100 Iteration 750| Training loss: 2.1649\n",
      "Epoch 31/100 Iteration 760| Training loss: 2.2093\n",
      "Epoch 31/100 Iteration 770| Training loss: 2.1764\n",
      "Epoch 32/100 Iteration 780| Training loss: 2.1673\n",
      "Epoch 32/100 Iteration 790| Training loss: 2.2080\n",
      "Epoch 32/100 Iteration 800| Training loss: 2.1497\n",
      "Epoch 33/100 Iteration 810| Training loss: 2.1882\n",
      "Epoch 33/100 Iteration 820| Training loss: 2.1618\n",
      "Epoch 34/100 Iteration 830| Training loss: 2.1507\n",
      "Epoch 34/100 Iteration 840| Training loss: 2.1822\n",
      "Epoch 34/100 Iteration 850| Training loss: 2.1292\n",
      "Epoch 35/100 Iteration 860| Training loss: 2.1644\n",
      "Epoch 35/100 Iteration 870| Training loss: 2.1395\n",
      "Epoch 36/100 Iteration 880| Training loss: 2.1437\n",
      "Epoch 36/100 Iteration 890| Training loss: 2.1712\n",
      "Epoch 36/100 Iteration 900| Training loss: 2.1161\n",
      "Epoch 37/100 Iteration 910| Training loss: 2.1592\n",
      "Epoch 37/100 Iteration 920| Training loss: 2.1193\n",
      "Epoch 38/100 Iteration 930| Training loss: 2.1193\n",
      "Epoch 38/100 Iteration 940| Training loss: 2.1614\n",
      "Epoch 38/100 Iteration 950| Training loss: 2.0954\n",
      "Epoch 39/100 Iteration 960| Training loss: 2.1406\n",
      "Epoch 39/100 Iteration 970| Training loss: 2.1091\n",
      "Epoch 40/100 Iteration 980| Training loss: 2.1120\n",
      "Epoch 40/100 Iteration 990| Training loss: 2.1477\n",
      "Epoch 40/100 Iteration 1000| Training loss: 2.0889\n",
      "Epoch 41/100 Iteration 1010| Training loss: 2.1272\n",
      "Epoch 41/100 Iteration 1020| Training loss: 2.1010\n",
      "Epoch 42/100 Iteration 1030| Training loss: 2.0941\n",
      "Epoch 42/100 Iteration 1040| Training loss: 2.1252\n",
      "Epoch 42/100 Iteration 1050| Training loss: 2.0620\n",
      "Epoch 43/100 Iteration 1060| Training loss: 2.1174\n",
      "Epoch 43/100 Iteration 1070| Training loss: 2.0911\n",
      "Epoch 44/100 Iteration 1080| Training loss: 2.0912\n",
      "Epoch 44/100 Iteration 1090| Training loss: 2.1240\n",
      "Epoch 44/100 Iteration 1100| Training loss: 2.0563\n",
      "Epoch 45/100 Iteration 1110| Training loss: 2.1108\n",
      "Epoch 45/100 Iteration 1120| Training loss: 2.0707\n",
      "Epoch 46/100 Iteration 1130| Training loss: 2.0760\n",
      "Epoch 46/100 Iteration 1140| Training loss: 2.1040\n",
      "Epoch 46/100 Iteration 1150| Training loss: 2.0535\n",
      "Epoch 47/100 Iteration 1160| Training loss: 2.0904\n",
      "Epoch 47/100 Iteration 1170| Training loss: 2.0622\n",
      "Epoch 48/100 Iteration 1180| Training loss: 2.0678\n",
      "Epoch 48/100 Iteration 1190| Training loss: 2.0798\n",
      "Epoch 48/100 Iteration 1200| Training loss: 2.0362\n",
      "Epoch 49/100 Iteration 1210| Training loss: 2.0773\n",
      "Epoch 49/100 Iteration 1220| Training loss: 2.0467\n",
      "Epoch 50/100 Iteration 1230| Training loss: 2.0538\n",
      "Epoch 50/100 Iteration 1240| Training loss: 2.0727\n",
      "Epoch 50/100 Iteration 1250| Training loss: 2.0239\n",
      "Epoch 51/100 Iteration 1260| Training loss: 2.0662\n",
      "Epoch 51/100 Iteration 1270| Training loss: 2.0413\n",
      "Epoch 52/100 Iteration 1280| Training loss: 2.0418\n",
      "Epoch 52/100 Iteration 1290| Training loss: 2.0818\n",
      "Epoch 52/100 Iteration 1300| Training loss: 1.9960\n",
      "Epoch 53/100 Iteration 1310| Training loss: 2.0403\n",
      "Epoch 53/100 Iteration 1320| Training loss: 2.0227\n",
      "Epoch 54/100 Iteration 1330| Training loss: 2.0321\n",
      "Epoch 54/100 Iteration 1340| Training loss: 2.0536\n",
      "Epoch 54/100 Iteration 1350| Training loss: 2.0058\n",
      "Epoch 55/100 Iteration 1360| Training loss: 2.0396\n",
      "Epoch 55/100 Iteration 1370| Training loss: 2.0151\n",
      "Epoch 56/100 Iteration 1380| Training loss: 2.0206\n",
      "Epoch 56/100 Iteration 1390| Training loss: 2.0445\n",
      "Epoch 56/100 Iteration 1400| Training loss: 1.9933\n",
      "Epoch 57/100 Iteration 1410| Training loss: 2.0333\n",
      "Epoch 57/100 Iteration 1420| Training loss: 2.0013\n",
      "Epoch 58/100 Iteration 1430| Training loss: 2.0071\n",
      "Epoch 58/100 Iteration 1440| Training loss: 2.0331\n",
      "Epoch 58/100 Iteration 1450| Training loss: 1.9834\n",
      "Epoch 59/100 Iteration 1460| Training loss: 2.0232\n",
      "Epoch 59/100 Iteration 1470| Training loss: 1.9945\n",
      "Epoch 60/100 Iteration 1480| Training loss: 2.0166\n",
      "Epoch 60/100 Iteration 1490| Training loss: 2.0354\n",
      "Epoch 60/100 Iteration 1500| Training loss: 1.9681\n",
      "Epoch 61/100 Iteration 1510| Training loss: 2.0103\n",
      "Epoch 61/100 Iteration 1520| Training loss: 1.9861\n",
      "Epoch 62/100 Iteration 1530| Training loss: 2.0025\n",
      "Epoch 62/100 Iteration 1540| Training loss: 2.0276\n",
      "Epoch 62/100 Iteration 1550| Training loss: 1.9653\n",
      "Epoch 63/100 Iteration 1560| Training loss: 1.9887\n",
      "Epoch 63/100 Iteration 1570| Training loss: 1.9795\n",
      "Epoch 64/100 Iteration 1580| Training loss: 1.9889\n",
      "Epoch 64/100 Iteration 1590| Training loss: 2.0143\n",
      "Epoch 64/100 Iteration 1600| Training loss: 1.9528\n",
      "Epoch 65/100 Iteration 1610| Training loss: 1.9916\n",
      "Epoch 65/100 Iteration 1620| Training loss: 1.9653\n",
      "Epoch 66/100 Iteration 1630| Training loss: 1.9810\n",
      "Epoch 66/100 Iteration 1640| Training loss: 2.0124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 Iteration 1650| Training loss: 1.9507\n",
      "Epoch 67/100 Iteration 1660| Training loss: 1.9921\n",
      "Epoch 67/100 Iteration 1670| Training loss: 1.9638\n",
      "Epoch 68/100 Iteration 1680| Training loss: 1.9714\n",
      "Epoch 68/100 Iteration 1690| Training loss: 1.9993\n",
      "Epoch 68/100 Iteration 1700| Training loss: 1.9398\n",
      "Epoch 69/100 Iteration 1710| Training loss: 1.9765\n",
      "Epoch 69/100 Iteration 1720| Training loss: 1.9607\n",
      "Epoch 70/100 Iteration 1730| Training loss: 1.9747\n",
      "Epoch 70/100 Iteration 1740| Training loss: 1.9814\n",
      "Epoch 70/100 Iteration 1750| Training loss: 1.9336\n",
      "Epoch 71/100 Iteration 1760| Training loss: 1.9786\n",
      "Epoch 71/100 Iteration 1770| Training loss: 1.9470\n",
      "Epoch 72/100 Iteration 1780| Training loss: 1.9633\n",
      "Epoch 72/100 Iteration 1790| Training loss: 1.9896\n",
      "Epoch 72/100 Iteration 1800| Training loss: 1.9368\n",
      "Epoch 73/100 Iteration 1810| Training loss: 1.9745\n",
      "Epoch 73/100 Iteration 1820| Training loss: 1.9420\n",
      "Epoch 74/100 Iteration 1830| Training loss: 1.9534\n",
      "Epoch 74/100 Iteration 1840| Training loss: 1.9707\n",
      "Epoch 74/100 Iteration 1850| Training loss: 1.9220\n",
      "Epoch 75/100 Iteration 1860| Training loss: 1.9546\n",
      "Epoch 75/100 Iteration 1870| Training loss: 1.9429\n",
      "Epoch 76/100 Iteration 1880| Training loss: 1.9429\n",
      "Epoch 76/100 Iteration 1890| Training loss: 1.9674\n",
      "Epoch 76/100 Iteration 1900| Training loss: 1.9105\n",
      "Epoch 77/100 Iteration 1910| Training loss: 1.9602\n",
      "Epoch 77/100 Iteration 1920| Training loss: 1.9336\n",
      "Epoch 78/100 Iteration 1930| Training loss: 1.9287\n",
      "Epoch 78/100 Iteration 1940| Training loss: 1.9566\n",
      "Epoch 78/100 Iteration 1950| Training loss: 1.9148\n",
      "Epoch 79/100 Iteration 1960| Training loss: 1.9462\n",
      "Epoch 79/100 Iteration 1970| Training loss: 1.9179\n",
      "Epoch 80/100 Iteration 1980| Training loss: 1.9328\n",
      "Epoch 80/100 Iteration 1990| Training loss: 1.9557\n",
      "Epoch 80/100 Iteration 2000| Training loss: 1.8945\n",
      "Epoch 81/100 Iteration 2010| Training loss: 1.9382\n",
      "Epoch 81/100 Iteration 2020| Training loss: 1.9312\n",
      "Epoch 82/100 Iteration 2030| Training loss: 1.9247\n",
      "Epoch 82/100 Iteration 2040| Training loss: 1.9349\n",
      "Epoch 82/100 Iteration 2050| Training loss: 1.8951\n",
      "Epoch 83/100 Iteration 2060| Training loss: 1.9360\n",
      "Epoch 83/100 Iteration 2070| Training loss: 1.9151\n",
      "Epoch 84/100 Iteration 2080| Training loss: 1.9076\n",
      "Epoch 84/100 Iteration 2090| Training loss: 1.9438\n",
      "Epoch 84/100 Iteration 2100| Training loss: 1.8929\n",
      "Epoch 85/100 Iteration 2110| Training loss: 1.9317\n",
      "Epoch 85/100 Iteration 2120| Training loss: 1.8910\n",
      "Epoch 86/100 Iteration 2130| Training loss: 1.9030\n",
      "Epoch 86/100 Iteration 2140| Training loss: 1.9235\n",
      "Epoch 86/100 Iteration 2150| Training loss: 1.8893\n",
      "Epoch 87/100 Iteration 2160| Training loss: 1.9197\n",
      "Epoch 87/100 Iteration 2170| Training loss: 1.8966\n",
      "Epoch 88/100 Iteration 2180| Training loss: 1.9187\n",
      "Epoch 88/100 Iteration 2190| Training loss: 1.9325\n",
      "Epoch 88/100 Iteration 2200| Training loss: 1.8781\n",
      "Epoch 89/100 Iteration 2210| Training loss: 1.9205\n",
      "Epoch 89/100 Iteration 2220| Training loss: 1.8862\n",
      "Epoch 90/100 Iteration 2230| Training loss: 1.9033\n",
      "Epoch 90/100 Iteration 2240| Training loss: 1.9308\n",
      "Epoch 90/100 Iteration 2250| Training loss: 1.8701\n",
      "Epoch 91/100 Iteration 2260| Training loss: 1.9056\n",
      "Epoch 91/100 Iteration 2270| Training loss: 1.8804\n",
      "Epoch 92/100 Iteration 2280| Training loss: 1.8898\n",
      "Epoch 92/100 Iteration 2290| Training loss: 1.9166\n",
      "Epoch 92/100 Iteration 2300| Training loss: 1.8634\n",
      "Epoch 93/100 Iteration 2310| Training loss: 1.8942\n",
      "Epoch 93/100 Iteration 2320| Training loss: 1.8734\n",
      "Epoch 94/100 Iteration 2330| Training loss: 1.8871\n",
      "Epoch 94/100 Iteration 2340| Training loss: 1.9090\n",
      "Epoch 94/100 Iteration 2350| Training loss: 1.8549\n",
      "Epoch 95/100 Iteration 2360| Training loss: 1.9092\n",
      "Epoch 95/100 Iteration 2370| Training loss: 1.8849\n",
      "Epoch 96/100 Iteration 2380| Training loss: 1.8862\n",
      "Epoch 96/100 Iteration 2390| Training loss: 1.8969\n",
      "Epoch 96/100 Iteration 2400| Training loss: 1.8471\n",
      "Epoch 97/100 Iteration 2410| Training loss: 1.8850\n",
      "Epoch 97/100 Iteration 2420| Training loss: 1.8766\n",
      "Epoch 98/100 Iteration 2430| Training loss: 1.8814\n",
      "Epoch 98/100 Iteration 2440| Training loss: 1.9021\n",
      "Epoch 98/100 Iteration 2450| Training loss: 1.8480\n",
      "Epoch 99/100 Iteration 2460| Training loss: 1.8863\n",
      "Epoch 99/100 Iteration 2470| Training loss: 1.8604\n",
      "Epoch 100/100 Iteration 2480| Training loss: 1.8657\n",
      "Epoch 100/100 Iteration 2490| Training loss: 1.9017\n",
      "Epoch 100/100 Iteration 2500| Training loss: 1.8437\n"
     ]
    }
   ],
   "source": [
    "rnn.train(train_x, train_y,\n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "----------------------------------------------------\n",
      "INFO:tensorflow:Restoring parameters from ./model-100/language_modeling.ckpt\n",
      "The wall steene,\n",
      "When in the masse seate tis moning, bore as it thes,\n",
      "That they so bathers of tile to seene and the ment\n",
      "\n",
      "   Ham. Nor then, as is the word some the winge,\n",
      "In tauth of hem the frander: and borthele\n",
      "With all teere\n",
      "\n",
      "   Hor. Thy the his sulle sis thim soule of the mast if\n",
      "Wat it it thisers if my tore in inthing stoone, thingers\n",
      "Toreete and the pases hare in this will to the math of,\n",
      "And and thime ho steed of is, att thas as to heare:\n",
      "I howe will the Singion and tomy alath and\n",
      "Whit oure to touen thing andelles, and thouelesse,\n",
      "And with that shand wording tell thing,\n",
      "Thes word seale in orring hate ant moure,\n",
      "The whall state, asting the mostes hing,\n",
      "Thing to the this mane, how seefe, hit would,\n",
      "And and thy thound, and whin sheadens,\n",
      "In hare ow the the tonten\n",
      "\n",
      "   Lair. Here so beand as im.\n",
      "This weld some my tores toe him areesint\n",
      "To here the Sints. That shelf in the farre thee a mes the to,\n",
      "That with in man anghen so fing indo the foldersers\n",
      "With all hes selland some off ile is a fo\n"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(rnn.sample(ckpt_dir='./model-100/', output_length=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
