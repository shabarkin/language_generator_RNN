{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "class SentimentRNN:\n",
    "    def __init__(self, n_words, seq_len = 200,\n",
    "                 lstm_size = 256, num_layers = 1,\n",
    "                 batch_size = 64, learning_rate = 0.0001,\n",
    "                 embed_size = 200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape = (self.batch_size, self.seq_len),\n",
    "                              name = \"tf_x\")\n",
    "        \n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                              shape = (self.batch_size),\n",
    "                              name = \"tf_y\")\n",
    "            \n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                              name = \"tf_keepprob\")\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform(\n",
    "                                (self.n_words,self.embed_size),\n",
    "                                    minval = -1, maxval = 1\n",
    "                                ), name = \"embedding\")\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(embedding,tf_x,name=\"embeded_x\")\n",
    "        \n",
    "        \n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob = tf_keepprob)\n",
    "             for i in range(self.num_layers)])\n",
    "        \n",
    "        self.initial_state = cells.zero_state(self.batch_size,tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "        \n",
    "        lstm_outputs,self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells,embed_x,\n",
    "            initial_state = self.initial_state\n",
    "        )\n",
    "        \n",
    "        logits = tf.layers.dense(inputs = lstm_outputs[:,-1],\n",
    "                                 units = 1,\n",
    "                                 activation = None,\n",
    "                                 name = \"logits\")\n",
    "        \n",
    "        logits = tf.squeeze(logits,name = \"logits_squeezed\")\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits,name = \"probabilities\")\n",
    "        \n",
    "        predictions = {\n",
    "            \"probabilities\": y_proba,\n",
    "            \"labels\": tf.cast(tf.round(y_proba),tf.int32,\n",
    "                             name = \"labels\")\n",
    "            \n",
    "        }\n",
    "        \n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels = tf_y,logits = logits),\n",
    "            name = \"cost\")\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost,name = \"train_op\")\n",
    "            \n",
    "    def train(self,x_train,y_train,n_epochs):\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(n_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                x_train, y_train, self.batch_size):\n",
    "                    \n",
    "                    feed = {\n",
    "                        \"tf_x:0\": batch_x,\n",
    "                        \"tf_y:0\": batch_y,\n",
    "                        \"tf_keepprob:0\": 0.5,\n",
    "                        self.initial_state: state \n",
    "                    }\n",
    "                    \n",
    "                    loss , _ , state = sess.run(\n",
    "                        [\"cost:0\",\"train_op\",self.final_state],\n",
    "                        feed_dict = feed\n",
    "                    )\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, n_epochs,\n",
    "                               iteration, loss))\n",
    "                    \n",
    "                    iteration +=1\n",
    "                    \n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "\n",
    "    def predict(self,x_data,return_proba = False):\n",
    "        preds = []\n",
    "        \n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess,tf.train.latest_checkpoint(\"./model/\")\n",
    "            )\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            \n",
    "            for ii, batch_x in enumerate(create_batch_generator(\n",
    "                    x_data, None, batch_size=self.batch_size), 1):\n",
    "                \n",
    "                feed = {\n",
    "                    \"tf_x:0\": batch_x,\n",
    "                    \"tf_keepprob:0\": 1.0,\n",
    "                    self.initial_state: test_state\n",
    "                }\n",
    "\n",
    "                if return_proba:\n",
    "                    pred,test_state = sess.run(\n",
    "                    [\"probabilities:0\",self.final_state],\n",
    "                    feed_dict = feed\n",
    "                    )\n",
    "                else:\n",
    "                    pred,test_state = sess.run(\n",
    "                    [\"labels:0\",self.final_state],\n",
    "                    feed_dict = feed\n",
    "                    )\n",
    "                preds.append(pred)\n",
    "            \n",
    "        return np.concatenate(preds)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd \n",
    "from string import punctuation \n",
    "import re \n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\",encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:06:46\n",
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "    \n",
    "    \n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()\n",
    "    \n",
    "\n",
    "    \n",
    "sequence_length = 200  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:2500, :]\n",
    "y_train = df.loc[:2500, 'sentiment'].values\n",
    "x_test = sequences[2500:, :]\n",
    "y_test = df.loc[2500:, 'sentiment'].values\n",
    "\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) +1\n",
    "\n",
    "rnn = SentimentRNN(n_words = n_words,\n",
    "                   seq_len = sequence_length,\n",
    "                   embed_size = 256,\n",
    "                   lstm_size = 128,\n",
    "                   num_layers = 1,\n",
    "                   batch_size = 100,\n",
    "                   learning_rate = 0.001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.68492\n",
      "Epoch: 2/40 Iteration: 40 | Train loss: 0.57580\n",
      "Epoch: 3/40 Iteration: 60 | Train loss: 0.45202\n",
      "Epoch: 4/40 Iteration: 80 | Train loss: 0.38675\n",
      "Epoch: 4/40 Iteration: 100 | Train loss: 0.37868\n",
      "Epoch: 5/40 Iteration: 120 | Train loss: 0.20784\n",
      "Epoch: 6/40 Iteration: 140 | Train loss: 0.21030\n",
      "Epoch: 7/40 Iteration: 160 | Train loss: 0.09901\n",
      "Epoch: 8/40 Iteration: 180 | Train loss: 0.06516\n",
      "Epoch: 8/40 Iteration: 200 | Train loss: 0.07589\n",
      "Epoch: 9/40 Iteration: 220 | Train loss: 0.02497\n",
      "Epoch: 10/40 Iteration: 240 | Train loss: 0.13567\n",
      "Epoch: 11/40 Iteration: 260 | Train loss: 0.08573\n",
      "Epoch: 12/40 Iteration: 280 | Train loss: 0.02687\n"
     ]
    }
   ],
   "source": [
    "rnn.train(x_train,y_train,n_epochs = 40)\n",
    "\n",
    "# Too long i decided to do it later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn.predict(x_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))\n",
    "\n",
    "\n",
    "proba = rnn.predict(x_test, return_proba=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
